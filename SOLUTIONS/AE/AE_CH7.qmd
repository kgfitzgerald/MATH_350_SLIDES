---
title: "AE-CH7"
author: "Your Name Here"
format: 
  html:
    self-contained: true
    toc: true
    toc_float: true
    number_section: false
    highlight: "tango"
    theme: "cosmo"
    df-print: paged
editor: visual
editor_options: 
  chunk_output_type: console
---

This Application Exercise walks through the guided activities in Chapter 7 of the textbook. There are spaces for you to answer each question (with code and/or text). Feel free to also use this document to add your own notes along the way.

## Load packages & data

```{r}
#| label: load-packages-data
#| message: false
#| warning: false

library(tidyverse)
library(janitor)
library(ggridges)
library(broom)
library(cowplot)
#library(lme4)

shuttle <- read_csv("./data/C7 Shuttle.csv") %>% 
  clean_names()
glimpse(shuttle)
```

## Question 1

## Question 2

```{r}
ggplot(shuttle, aes(x = temperature,
                    y = success)) +
  geom_point()

ggplot(shuttle, aes(x = temperature,
                    y = factor(success))) +
  geom_boxplot()

ggplot(shuttle, aes(x = temperature,
                    y = factor(success))) +
  geom_density_ridges()
```

## Question 3

```{r}
ggplot(shuttle, aes(x = temperature,
                    y = success)) +
  geom_point() +
  geom_smooth(method = "lm")

m1 <- lm(success ~ temperature, 
         data = shuttle)
tidy(m1)

-1.90 + 0.0374*60
-1.90 + 0.0374*85
```

What's the problem with these predictions?

\[Skip Q4 - done in class together\]

## Question 5

```{r}


create_plot <- function(beta0, beta1){
  x <- seq(0, 35, 0.01)
  y <- exp(beta0 + beta1*x)/(1 + exp(beta0 + beta1*x))

  data <- data.frame(x, y)
  ggplot(data, aes(x = x, y = y)) +
    geom_line() +
    labs(title = paste("beta0 = ", beta0,
                       ", beta1 = ",
                       beta1, sep = ""))
}
p1 <- create_plot(-10, 0.5)
p2 <- create_plot(-5, 0.5)
p3 <- create_plot(-10, 1)
p4 <- create_plot(-5, 1)
p5 <- create_plot(-10, 1.5)
p6 <- create_plot(-5, 1.5)
plot_grid(p1, p3, p5, p2, p4, p6, nrow = 2)

p1 <- create_plot(10, -0.5)
p2 <- create_plot(5, -0.5)
p3 <- create_plot(10, -1)
p4 <- create_plot(5, -1)
p5 <- create_plot(10, -1.5)
p6 <- create_plot(5, -1.5)
plot_grid(p1, p3, p5, p2, p4, p6, nrow = 2)
```

## Question 6

```{r}
m2 <- glm(success ~ temperature, 
          family = "binomial",
          data = shuttle)
tidy(m2)
```

## Question 7

$$\hat{\pi}\\i = \frac{e^{b_0+b_1x_i}}{1 + e^{b_0+b_1x_i}}$$

```{r}
b0 <- tidy(m2)[1,2] %>% pull()
b1 <- tidy(m2)[2,2] %>% pull()
x <- c(31,50,75)
pi_hat <- exp(b0 + b1*x)/(1 + exp(b0 + b1*x))
pi_hat
```

## Question 8

```{r}
x <- c(60, 70)
pi_hat <- exp(b0 + b1*x)/(1 + exp(b0 + b1*x))
odds <- pi_hat/(1 - pi_hat)
odds
```

## Question 9

```{r}
exp(10*b1)
```

## Question 10

> We do not expect the same difference between 52 and 51 because:
>
> Odds(60) - Odds(59) = (exp(b1)-1)\*Odds(59)
>
> Odds(52) - Odds(51) = (exp(b1) - 1)\*Odds(51)
>
> In OLS, an increase in one unit would always result in an increase in the response by a constant value, *b*<sub>1</sub>, but in logistic regression the change in odds is based on multiplication. Since Odds(59) is not equal to Odds(51), the increase is not constant.

## Question 11

```{r, warning = FALSE}
b0_ols <- tidy(m1)[1,2] %>% pull()
b1_ols <- tidy(m1)[2,2] %>% pull()

b0_mle <- tidy(m2)[1,2] %>% pull()
b1_mle <- tidy(m2)[2,2] %>% pull()

data <- data.frame(x = seq(0,100, 1)) %>% 
  mutate(y_ols = exp(b0_ols + b1_ols*x)/(1 + exp(b0_ols + b1_ols*x)),
         y_mle = exp(b0_mle + b1_mle*x)/(1 + exp(b0_mle + b1_mle*x)))

ggplot(data = data) +
  geom_line(aes(x = x, y = y_ols), 
            color = "red") +
  geom_label(aes(x = 100, y = max(y_ols),
                 label = "OLS")) +
  geom_line(aes(x = x, y = y_mle), 
            color = "blue") +
  geom_label(aes(x = 100, y = max(y_mle),
                 label = "MLE")) +
  labs(x = "Temperature",
       y = "Probability of success",
       title = "MLE vs OLS estimation")


```

## Question 12

```{r}
se_b1 <- tidy(m2)[2,3] %>% pull()
Z <- b1_mle / se_b1
Z
Z_star <- qnorm(.975) #95% CI
exp(b1_mle - Z_star*se_b1)
exp(b1_mle + Z_star*se_b1)
```

## Question 13

### Parts a - b

```{r}
shuttle <- shuttle %>% 
  mutate(success_recoded = if_else(success == 1, 
                                   0, 1))
m3 <- glm(success_recoded ~ temperature, 
          family = "binomial",
          data = shuttle)
tidy(m3)
```

> When a "success" is designated with a 0, the signs of the parameter estimates are switched

### Part c

Odds(temp + 1) = exp(-.232)xOdds(temp) = 0.793xOdds(temp), which is the inverse of the odds ration from the previous model.

```{r}
b0_m3 <- tidy(m3)[1,2] %>% pull()
b1_m3 <- tidy(m3)[2,2] %>% pull()
x <- c(60,70)
pi_hat_m3 <- exp(b0_m3 + b1_m3*x)/(1 + exp(b0_m3 + b1_m3*x))
pi_hat_m3
odds_m3 <- pi_hat_m3/(1 - pi_hat_m3)
odds_m3
1/odds
```

## Question 14

$$H_0: \text{Restricted model: } \pi_i = \frac{e^{\beta_0}}{1 + e^{\beta_0}}$$

$$H_A: \text{Full model: } \pi_i = \frac{e^{\beta_0 + \beta_1x_i}}{1 + e^{\beta_0 + \beta_1x_i}}$$

```{r}
summary <- summary(m2)
summary
G <- summary$null.deviance - summary$deviance
G
df <- 1
pchisq(G, df, lower.tail = FALSE)
```

> The small p-value indicates that the null hypothesis can be rejected. However, due to the small sample size we should be cautious about the reliability of the p-value for the LRT. 

## Question 15

### Part a

```{r, message = FALSE}
cancer <- read_csv("./data/C7 Cancer2.csv") %>% 
  clean_names()
glimpse(cancer)
m4 <- glm(malignant ~ radius + concavity, 
          family = "binomial",
          data = cancer)
tidy(m4)
```

### Part b

```{r}
summary <- summary(m4)
summary
G <- summary$null.deviance - summary$deviance
G
df <- 1
pchisq(G, df, lower.tail = FALSE)
```

### Part c

```{r}
b0 <- tidy(m4)[1,2] %>% pull()
b1 <- tidy(m4)[2,2] %>% pull()
b2 <- tidy(m4)[3,2] %>% pull()


#concave = 0
x <- c(4,0)
pi_hat <- exp(b0 + b1*x[1] + b2*x[2])/(1 + exp(b0 + b1*x[1] + b2*x[2]))
pi_hat

#concave = 1
x <- c(4,1)
pi_hat_m4 <- exp(b0 + b1*x[1] + b2*x[2])/(1 + exp(b0 + b1*x[1] + b2*x[2]))
pi_hat_m4
```


## Question 16

### Part a

```{r}
m5 <- glm(malignant ~ radius, 
          family = "binomial",
          data = cancer)
tidy(m5)
summary <- summary(m5)
summary
G <- summary$null.deviance - summary$deviance
G
df <- 1
pchisq(G, df, lower.tail = FALSE)
```

### Part b

```{r}
b0 <- tidy(m5)[1,2] %>% pull()
b1 <- tidy(m5)[2,2] %>% pull()
x <- 4
pi_hat <- exp(b0 + b1*x)/(1 + exp(b0 + b1*x))
pi_hat
```


## Question 17

```{r}
odds <- exp(b2)
odds
```

> After adjusting for radius, the odds of malignancy for cells with concave nuclei is estimated to be about 28 times larger than the odds of malignancy for cells with round nuclei. 

## Question 18

```{r}
cancer <- cancer %>% 
  mutate(benign = if_else(malignant == 1, 0, 1))
m6 <- glm(benign ~ radius + concavity, 
          family = "binomial",
          data = cancer)
tidy(m6)

b2 <- tidy(m6)[3,2] %>% pull()
odds <- exp(b2)
odds
```

> After adjusting for radius, the odds of a benign cell with concave nuclei is estimated to be about 4% of the odds of a benign cell with round nuclei. These odds are the inverse of the ones given in Question 17. 1/27.368 = 0.036